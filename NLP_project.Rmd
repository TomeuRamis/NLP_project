---
title: "NLP project - Bartomeu Ramis"
output:
  pdf_document: default
  html_notebook: default
---
```{r, warning=FALSE}
library(keras)
library(dplyr)
library(ggplot2)
library(tm)
library(corpus)
library(wordcloud)
require(quanteda)
require(quanteda.textmodels)
library(quanteda.textplots)
require(caret)
```


Load data set
This data set includes 23486 rows and 10 feature variables. Each row corresponds to a customer review, and includes the variables:

    Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.
    Age: Positive Integer variable of the reviewers age.
    Title: String variable for the title of the review.
    Review Text: String variable for the review body.
    Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.
    Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.
    Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.
    Division Name: Categorical name of the product high level division.
    Department Name: Categorical name of the product department name.
    Class Name: Categorical name of the product class name.
    
```{r}
set.seed(222)
data = read.csv("Womens Clothing E-Commerce Reviews.csv", header=TRUE)
head(data)
```

So, first of all, lets drop those columns that we wont be needing. 
- Clothing ID could be useful if we were interested in which clothes have better opinions, but in our case, we don't really care. 
- Age is another piece of information that, for our purpose, we don't really need.
- Rating would be extremely useful if we had not the recommend IND. However, we will keep it, just in case. 
- Positive feedback can also be omitted, given the fact that represents the opinion of other customers on the review, and gives no insight into the review's expressed opinions. 
- And finally, Division Name, Department Name and Class name wont bring any useful information for our objectives.

```{r}
data = subset(data, select=c(X, Title, Review.Text, Rating, Recommended.IND))
names(data)[names(data) == "X"] <- "id"
names(data)[names(data) == "Review.Text"] <- "Text"
names(data)[names(data) == "Recommended.IND"] <- "Recommend"
summary(data)
```
The title of the review has also some key words that will be quite helpful to determinate the "feelings", so in order to simplify the learning process we will merge the titled and the review text in a new column named "all_text".
```{r}
data$all_text <-paste(data$Title,data$Text, sep=" ")
head(data)
```
lets look for null values
```{r}
summary(data)
cat("number of na/nan values: ", sum(is.na(data)),"\n")
cat("number of na/nan values for 'all_text' column: ",sum(is.na(data$all_text)), "\n")
cat("number of na/nan values for 'recommend' column: ",sum(is.na(data$Recommend)), "\n")
```
It seem that the data is already clean from missing values.
Next let's look for outliers or non valid values.
```{r}
cat("number of values bigger than 1 or smaller than 0 for Recommend: ", sum(data$Recommend > 1 | data$Recommend < 0), "\n")
cat("number of values smaller than 1 and bigger than 0 for Recommend: ",sum(data$Recommend < 1 & data$Recommend > 0), "\n")
ggplot(data, aes(x=Rating)) + geom_histogram(color="darkblue", fill="lightblue")
```
#Exploratory analisis
```{r}
df <- data.frame(
  group = c("1", "2", "3", "4", "5"),
  value = c(sum(data$Rating==1), sum(data$Rating==2), sum(data$Rating==2),sum(data$Rating==4),sum(data$Rating==4))
  )
 ggplot(df, aes(x="", y=value, fill=group)) + geom_bar(width = 1, stat = "identity")+ coord_polar("y", start=0)+ scale_fill_brewer(palette="Blues")+
  theme_minimal()
```
```{r}
df <- data.frame(
  group = c("recommends", "not recommend"),
  value = c(sum(data$Recommend==1), sum(data$Recommend==0))
  )
 ggplot(df, aes(x="", y=value, fill=group)) + geom_bar(width = 1, stat = "identity")+ coord_polar("y", start=0)+ scale_fill_brewer(palette="Reds")+
  theme_minimal()
```
# Text clearing: normalization, removing non-letter characters, removing stopwords and stemming 
```{r}
summary(data$corpus <- corpus(data$all_text),10)
data$tokens = tokens(data$corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE)
#data$dfm = dfm(data$tokens, remove = stopwords("english"), tolower=TRUE, stem = TRUE)
data$dfm = dfm(data$tokens, tolower=TRUE) %>% dfm_remove( stopwords_en) %>% dfm_wordstem()
summary(data$dfm)
```
```{r}
topfeatures(data$dfm, 20) # 20 most frequent words
textplot_wordcloud(data$dfm, random_order = FALSE,
                  rotation = .25,
                  color = RColorBrewer::brewer.pal(8, "Dark2"))

```


```{r}
train_index <- createDataPartition(data$id, p = .8, list = FALSE, times = 1)
train <- subset(data, data$id %in% train_index)
`%notin%` <- Negate(`%in%`)
test <- subset(data, data$id %notin% train_index)
```


```{r}
#Naive Bayes text model with prior distribution on text set to 'termfreq'
nb_model <- textmodel_nb(train$dfm, train$Recommend, smooth=1, prior="termfreq")
summary(nb_model)

prediction = predict(nb_model, newdata = test$dfm)
sum(prediction == test$Recommend)/count(test)
```
```{r}
#Naive Bayes text model with prior distribution on text set to 'uniform'
nb_model_uniform <- textmodel_nb(train$dfm, train$Recommend, smooth=1, prior="uniform")

prediction = predict(nb_model_uniform, newdata = test$dfm)
sum(prediction == test$Recommend)/count(test)
```
```{r}
#Naive Bayes text model with prior distribution on text set to 'docfreq'
nb_model_docfreq <- textmodel_nb(train$dfm, train$Recommend, smooth=1, prior="docfreq")

prediction = predict(nb_model_docfreq, newdata = test$dfm)
sum(prediction == test$Recommend)/count(test)
```

```{r}
#separate positivo from negative
data_reduced <- subset(data, select=-c(corpus,tokens,dfm))
pos <- data_reduced[data_reduced$Recommend==1,]
neg <- data_reduced[data_reduced$Recommend==0,]

i = as.integer(count(neg)$n[1])
reduced_pos <- pos[sample(1:nrow(pos), size = i), ]   

balanced_data = bind_rows(reduced_pos, neg)
```

```{r}
df <- data.frame(
  group = c("recommends", "not recommend"),
  value = c(sum(balanced_data$Recommend==1), sum(data$Recommend==0))
  )
 ggplot(df, aes(x="", y=value, fill=group)) + geom_bar(width = 1, stat = "identity") + coord_polar("y", start=0) + scale_fill_brewer(palette="Reds") + theme_minimal()
```
```{r}
balanced_data$corpus <- corpus(balanced_data$all_text)
balanced_data$tokens = tokens(balanced_data$corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE)
balanced_data$dfm = dfm(balanced_data$tokens, tolower=TRUE) %>% dfm_remove( stopwords_en) %>% dfm_wordstem()

train_index_b <- createDataPartition(balanced_data$id, p = .8, list = FALSE, times = 1)
train_b <- subset(balanced_data, balanced_data$id %in% train_index)
test_b <- subset(balanced_data, balanced_data$id %notin% train_index)
```
```{r}
#Naive Bayes text model with prior distribution on text set to 'docfreq'
nb_model_b <- textmodel_nb(train_b$dfm, train_b$Recommend, smooth=1, prior="docfreq")
summary(nb_model_b)

prediction = predict(nb_model_b, newdata = test_b$dfm)
sum(prediction == test_b$Recommend)/count(test_b)
```
```{r}
confusionMatrix(prediction, factor(test_b$Recommend))
```
```{r}
precision <- 679/(679+97)
recall <- 679/(679+138)
cat("precision: ",precision,", recall: ",recall)
```


#References
https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews
https://www.marsja.se/how-to-concatenate-two-columns-or-more-in-r-stringr-tidyr/
https://discuss.analyticsvidhya.com/t/how-to-count-the-missing-value-in-r/2949/4
https://www.r-graph-gallery.com/index.html
https://www.r-bloggers.com/2021/05/sentiment-analysis-in-r-3/
https://cran.r-project.org/web/packages/corpus/vignettes/stemmer.html
https://quanteda.io/articles/quickstart.html#extracting-features-from-a-corpus-1   
https://www.journaldev.com/46732/confusion-matrix-in-r